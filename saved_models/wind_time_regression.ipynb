{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%d%m%Y%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacha\\Documents\\GitHub\\plymouth-university-proj518\n"
     ]
    }
   ],
   "source": [
    "# set working directory to project root\n",
    "os.chdir(\"C:/Users/zacha/Documents/GitHub/plymouth-university-proj518\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler():\n",
    "    file = \"data/NCEP daily velocity potential reanalysis.csv\"\n",
    "    df = pd.read_csv(file, sep=\",\")\n",
    "\n",
    "    x = df[[\"Lon\", \"Lat\", \"Day\"]]\n",
    "    y = df[[\"Chi\"]]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "\n",
    "    yscaler = StandardScaler()\n",
    "    yscaler = yscaler.fit(y)\n",
    "\n",
    "    # transform y_train and y_test to standardized scale\n",
    "    y_train = yscaler.transform(y_train)\n",
    "    y_test = yscaler.transform(y_test)\n",
    "\n",
    "    scaler_path = os.path.join(\n",
    "        os.getcwd(),\n",
    "        \"saved_models\",\n",
    "        f\"wind_time_scaler_{TIMESTAMP}\",\n",
    "    )\n",
    "    joblib.dump(yscaler, scaler_path)\n",
    "    print(\"Saved scaler at %s \" % scaler_path)\n",
    "\n",
    "    # normalise X input type\n",
    "    x_train = x_train.astype(\"float32\")\n",
    "    x_test = x_test.astype(\"float32\")\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, yscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(32, input_dim=3, activation=\"relu\"))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(512, activation=\"relu\"))  # NEW\n",
    "    model.add(Dense(1024, activation=\"relu\"))  # NEW\n",
    "    model.add(Dense(128, activation=\"relu\"))  # NEW\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"rmse\"]) #metrics=[\"mse\", \"mae\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    epochs=150,\n",
    "    batch_size=50,\n",
    "    verbose=1,\n",
    "    validation_split=0.2,\n",
    "):\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose,\n",
    "        validation_split=validation_split,\n",
    "    )\n",
    "\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"model loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # save model and weights\n",
    "    model_path = os.path.join(\n",
    "        os.getcwd(),\n",
    "        \"saved_models\",\n",
    "        f\"wind_time_regression_model_{TIMESTAMP}.h5\",\n",
    "    )\n",
    "    model.save(model_path)\n",
    "    print(\"Saved trained model at %s \" % model_path)\n",
    "\n",
    "    # score trained model.\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print(\"Test loss:\", scores[0])\n",
    "    print(\"Test accuracy:\", scores[1])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, yscaler, coords):\n",
    "    regression = model.predict(coords)\n",
    "    regression = yscaler.inverse_transform(\n",
    "        regression\n",
    "    )  # transform back to original scale\n",
    "\n",
    "    regression = pd.DataFrame(regression, columns=[\"Chi\"])\n",
    "    regression = pd.concat([coords, regression], axis=1)\n",
    "\n",
    "    print(regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scaler at C:\\Users\\zacha\\Documents\\GitHub\\plymouth-university-proj518\\saved_models\\wind_time_scaler_25082022164000 \n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 840,465\n",
      "Trainable params: 840,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "3270/3270 [==============================] - 25s 7ms/step - loss: 0.2572 - mse: 0.2572 - mae: 0.3847 - val_loss: 0.1769 - val_mse: 0.1769 - val_mae: 0.3267\n",
      "Epoch 2/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.1605 - mse: 0.1605 - mae: 0.3104 - val_loss: 0.1510 - val_mse: 0.1510 - val_mae: 0.3009\n",
      "Epoch 3/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.1448 - mse: 0.1448 - mae: 0.2936 - val_loss: 0.1427 - val_mse: 0.1427 - val_mae: 0.2911\n",
      "Epoch 4/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.1328 - mse: 0.1328 - mae: 0.2799 - val_loss: 0.1343 - val_mse: 0.1343 - val_mae: 0.2806\n",
      "Epoch 5/200\n",
      "3270/3270 [==============================] - 25s 7ms/step - loss: 0.1236 - mse: 0.1236 - mae: 0.2691 - val_loss: 0.1193 - val_mse: 0.1193 - val_mae: 0.2631\n",
      "Epoch 6/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.1159 - mse: 0.1159 - mae: 0.2599 - val_loss: 0.1107 - val_mse: 0.1107 - val_mae: 0.2531\n",
      "Epoch 7/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.1089 - mse: 0.1089 - mae: 0.2514 - val_loss: 0.1036 - val_mse: 0.1036 - val_mae: 0.2442\n",
      "Epoch 8/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.1029 - mse: 0.1029 - mae: 0.2439 - val_loss: 0.1043 - val_mse: 0.1043 - val_mae: 0.2455\n",
      "Epoch 9/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0970 - mse: 0.0970 - mae: 0.2364 - val_loss: 0.0941 - val_mse: 0.0941 - val_mae: 0.2326\n",
      "Epoch 10/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0924 - mse: 0.0924 - mae: 0.2305 - val_loss: 0.0904 - val_mse: 0.0904 - val_mae: 0.2278\n",
      "Epoch 11/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0881 - mse: 0.0881 - mae: 0.2247 - val_loss: 0.1006 - val_mse: 0.1006 - val_mae: 0.2381\n",
      "Epoch 12/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0845 - mse: 0.0845 - mae: 0.2197 - val_loss: 0.0847 - val_mse: 0.0847 - val_mae: 0.2206\n",
      "Epoch 13/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0815 - mse: 0.0815 - mae: 0.2156 - val_loss: 0.0836 - val_mse: 0.0836 - val_mae: 0.2186\n",
      "Epoch 14/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0785 - mse: 0.0785 - mae: 0.2113 - val_loss: 0.0767 - val_mse: 0.0767 - val_mae: 0.2093\n",
      "Epoch 15/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0763 - mse: 0.0763 - mae: 0.2080 - val_loss: 0.0829 - val_mse: 0.0829 - val_mae: 0.2161\n",
      "Epoch 16/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0740 - mse: 0.0740 - mae: 0.2047 - val_loss: 0.0698 - val_mse: 0.0698 - val_mae: 0.1990\n",
      "Epoch 17/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0718 - mse: 0.0718 - mae: 0.2015 - val_loss: 0.0683 - val_mse: 0.0683 - val_mae: 0.1961\n",
      "Epoch 18/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0700 - mse: 0.0700 - mae: 0.1988 - val_loss: 0.0725 - val_mse: 0.0725 - val_mae: 0.2014\n",
      "Epoch 19/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0683 - mse: 0.0683 - mae: 0.1961 - val_loss: 0.0999 - val_mse: 0.0999 - val_mae: 0.2341\n",
      "Epoch 20/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0669 - mse: 0.0669 - mae: 0.1939 - val_loss: 0.0682 - val_mse: 0.0682 - val_mae: 0.1974\n",
      "Epoch 21/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0653 - mse: 0.0653 - mae: 0.1914 - val_loss: 0.0658 - val_mse: 0.0658 - val_mae: 0.1935\n",
      "Epoch 22/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0642 - mse: 0.0642 - mae: 0.1899 - val_loss: 0.0612 - val_mse: 0.0612 - val_mae: 0.1857\n",
      "Epoch 23/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0625 - mse: 0.0625 - mae: 0.1873 - val_loss: 0.0583 - val_mse: 0.0583 - val_mae: 0.1808\n",
      "Epoch 24/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0614 - mse: 0.0614 - mae: 0.1855 - val_loss: 0.0637 - val_mse: 0.0637 - val_mae: 0.1896\n",
      "Epoch 25/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0600 - mse: 0.0600 - mae: 0.1833 - val_loss: 0.0589 - val_mse: 0.0589 - val_mae: 0.1812\n",
      "Epoch 26/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0591 - mse: 0.0591 - mae: 0.1817 - val_loss: 0.0620 - val_mse: 0.0620 - val_mae: 0.1855\n",
      "Epoch 27/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0582 - mse: 0.0582 - mae: 0.1803 - val_loss: 0.0645 - val_mse: 0.0645 - val_mae: 0.1910\n",
      "Epoch 28/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0570 - mse: 0.0570 - mae: 0.1783 - val_loss: 0.0620 - val_mse: 0.0620 - val_mae: 0.1868\n",
      "Epoch 29/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0560 - mse: 0.0560 - mae: 0.1767 - val_loss: 0.0669 - val_mse: 0.0669 - val_mae: 0.1915\n",
      "Epoch 30/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0554 - mse: 0.0554 - mae: 0.1757 - val_loss: 0.0673 - val_mse: 0.0673 - val_mae: 0.1920\n",
      "Epoch 31/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0545 - mse: 0.0545 - mae: 0.1742 - val_loss: 0.0555 - val_mse: 0.0555 - val_mae: 0.1763\n",
      "Epoch 32/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0536 - mse: 0.0536 - mae: 0.1726 - val_loss: 0.0582 - val_mse: 0.0582 - val_mae: 0.1815\n",
      "Epoch 33/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0528 - mse: 0.0528 - mae: 0.1712 - val_loss: 0.0541 - val_mse: 0.0541 - val_mae: 0.1738\n",
      "Epoch 34/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0522 - mse: 0.0522 - mae: 0.1701 - val_loss: 0.0488 - val_mse: 0.0488 - val_mae: 0.1652\n",
      "Epoch 35/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0517 - mse: 0.0517 - mae: 0.1694 - val_loss: 0.0757 - val_mse: 0.0757 - val_mae: 0.2002\n",
      "Epoch 36/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0508 - mse: 0.0508 - mae: 0.1679 - val_loss: 0.0507 - val_mse: 0.0507 - val_mae: 0.1681\n",
      "Epoch 37/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0500 - mse: 0.0500 - mae: 0.1664 - val_loss: 0.0508 - val_mse: 0.0508 - val_mae: 0.1691\n",
      "Epoch 38/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0494 - mse: 0.0494 - mae: 0.1654 - val_loss: 0.0478 - val_mse: 0.0478 - val_mae: 0.1630\n",
      "Epoch 39/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0488 - mse: 0.0488 - mae: 0.1643 - val_loss: 0.0500 - val_mse: 0.0500 - val_mae: 0.1670\n",
      "Epoch 40/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0483 - mse: 0.0483 - mae: 0.1635 - val_loss: 0.0501 - val_mse: 0.0501 - val_mae: 0.1677\n",
      "Epoch 41/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0479 - mse: 0.0479 - mae: 0.1627 - val_loss: 0.0499 - val_mse: 0.0499 - val_mae: 0.1660\n",
      "Epoch 42/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0475 - mse: 0.0475 - mae: 0.1620 - val_loss: 0.0482 - val_mse: 0.0482 - val_mae: 0.1638\n",
      "Epoch 43/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0466 - mse: 0.0466 - mae: 0.1604 - val_loss: 0.0459 - val_mse: 0.0459 - val_mae: 0.1589\n",
      "Epoch 44/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0463 - mse: 0.0463 - mae: 0.1599 - val_loss: 0.0471 - val_mse: 0.0471 - val_mae: 0.1615\n",
      "Epoch 45/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0462 - mse: 0.0462 - mae: 0.1596 - val_loss: 0.0476 - val_mse: 0.0476 - val_mae: 0.1626\n",
      "Epoch 46/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0455 - mse: 0.0455 - mae: 0.1584 - val_loss: 0.0472 - val_mse: 0.0472 - val_mae: 0.1613\n",
      "Epoch 47/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0452 - mse: 0.0452 - mae: 0.1578 - val_loss: 0.0503 - val_mse: 0.0503 - val_mae: 0.1678\n",
      "Epoch 48/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0449 - mse: 0.0449 - mae: 0.1571 - val_loss: 0.0440 - val_mse: 0.0440 - val_mae: 0.1565\n",
      "Epoch 49/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0445 - mse: 0.0445 - mae: 0.1564 - val_loss: 0.0459 - val_mse: 0.0459 - val_mae: 0.1596\n",
      "Epoch 50/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0445 - mse: 0.0445 - mae: 0.1564 - val_loss: 0.0467 - val_mse: 0.0467 - val_mae: 0.1603\n",
      "Epoch 51/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0438 - mse: 0.0438 - mae: 0.1552 - val_loss: 0.0455 - val_mse: 0.0455 - val_mae: 0.1589\n",
      "Epoch 52/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0436 - mse: 0.0436 - mae: 0.1547 - val_loss: 0.0433 - val_mse: 0.0433 - val_mae: 0.1537\n",
      "Epoch 53/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0431 - mse: 0.0431 - mae: 0.1538 - val_loss: 0.0414 - val_mse: 0.0414 - val_mae: 0.1515\n",
      "Epoch 54/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0427 - mse: 0.0427 - mae: 0.1530 - val_loss: 0.0440 - val_mse: 0.0440 - val_mae: 0.1543\n",
      "Epoch 55/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0423 - mse: 0.0423 - mae: 0.1522 - val_loss: 0.0464 - val_mse: 0.0464 - val_mae: 0.1596\n",
      "Epoch 56/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0421 - mse: 0.0421 - mae: 0.1520 - val_loss: 0.0463 - val_mse: 0.0463 - val_mae: 0.1597\n",
      "Epoch 57/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0419 - mse: 0.0419 - mae: 0.1514 - val_loss: 0.0422 - val_mse: 0.0422 - val_mae: 0.1535\n",
      "Epoch 58/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0416 - mse: 0.0416 - mae: 0.1507 - val_loss: 0.0436 - val_mse: 0.0436 - val_mae: 0.1548\n",
      "Epoch 59/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0413 - mse: 0.0413 - mae: 0.1502 - val_loss: 0.0415 - val_mse: 0.0415 - val_mae: 0.1508\n",
      "Epoch 60/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0411 - mse: 0.0411 - mae: 0.1497 - val_loss: 0.0442 - val_mse: 0.0442 - val_mae: 0.1556\n",
      "Epoch 61/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0407 - mse: 0.0407 - mae: 0.1491 - val_loss: 0.0511 - val_mse: 0.0511 - val_mae: 0.1664\n",
      "Epoch 62/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0404 - mse: 0.0404 - mae: 0.1485 - val_loss: 0.0459 - val_mse: 0.0459 - val_mae: 0.1568\n",
      "Epoch 63/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0401 - mse: 0.0401 - mae: 0.1479 - val_loss: 0.0456 - val_mse: 0.0456 - val_mae: 0.1570\n",
      "Epoch 64/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0402 - mse: 0.0402 - mae: 0.1481 - val_loss: 0.0425 - val_mse: 0.0425 - val_mae: 0.1529\n",
      "Epoch 65/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0395 - mse: 0.0395 - mae: 0.1468 - val_loss: 0.0384 - val_mse: 0.0384 - val_mae: 0.1454\n",
      "Epoch 66/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0394 - mse: 0.0394 - mae: 0.1466 - val_loss: 0.0397 - val_mse: 0.0397 - val_mae: 0.1471\n",
      "Epoch 67/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0393 - mse: 0.0393 - mae: 0.1465 - val_loss: 0.0424 - val_mse: 0.0424 - val_mae: 0.1521\n",
      "Epoch 68/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0389 - mse: 0.0389 - mae: 0.1455 - val_loss: 0.0386 - val_mse: 0.0386 - val_mae: 0.1458\n",
      "Epoch 69/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0387 - mse: 0.0387 - mae: 0.1451 - val_loss: 0.0402 - val_mse: 0.0402 - val_mae: 0.1488\n",
      "Epoch 70/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0384 - mse: 0.0384 - mae: 0.1446 - val_loss: 0.0386 - val_mse: 0.0386 - val_mae: 0.1453\n",
      "Epoch 71/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0386 - mse: 0.0386 - mae: 0.1447 - val_loss: 0.0394 - val_mse: 0.0394 - val_mae: 0.1467\n",
      "Epoch 72/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0381 - mse: 0.0381 - mae: 0.1439 - val_loss: 0.0369 - val_mse: 0.0369 - val_mae: 0.1417\n",
      "Epoch 73/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0378 - mse: 0.0378 - mae: 0.1433 - val_loss: 0.0371 - val_mse: 0.0371 - val_mae: 0.1417\n",
      "Epoch 74/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0378 - mse: 0.0378 - mae: 0.1433 - val_loss: 0.0380 - val_mse: 0.0380 - val_mae: 0.1443\n",
      "Epoch 75/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0375 - mse: 0.0375 - mae: 0.1427 - val_loss: 0.0467 - val_mse: 0.0467 - val_mae: 0.1596\n",
      "Epoch 76/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0374 - mse: 0.0374 - mae: 0.1423 - val_loss: 0.0408 - val_mse: 0.0408 - val_mae: 0.1490\n",
      "Epoch 77/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0371 - mse: 0.0371 - mae: 0.1420 - val_loss: 0.0375 - val_mse: 0.0375 - val_mae: 0.1431\n",
      "Epoch 78/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0369 - mse: 0.0369 - mae: 0.1415 - val_loss: 0.0432 - val_mse: 0.0432 - val_mae: 0.1526\n",
      "Epoch 79/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0369 - mse: 0.0369 - mae: 0.1414 - val_loss: 0.0557 - val_mse: 0.0557 - val_mae: 0.1727\n",
      "Epoch 80/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0367 - mse: 0.0367 - mae: 0.1410 - val_loss: 0.0388 - val_mse: 0.0388 - val_mae: 0.1449\n",
      "Epoch 81/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0366 - mse: 0.0366 - mae: 0.1407 - val_loss: 0.0390 - val_mse: 0.0390 - val_mae: 0.1462\n",
      "Epoch 82/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0363 - mse: 0.0363 - mae: 0.1402 - val_loss: 0.0362 - val_mse: 0.0362 - val_mae: 0.1413\n",
      "Epoch 83/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0360 - mse: 0.0360 - mae: 0.1397 - val_loss: 0.0401 - val_mse: 0.0401 - val_mae: 0.1475\n",
      "Epoch 84/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0359 - mse: 0.0359 - mae: 0.1393 - val_loss: 0.0354 - val_mse: 0.0354 - val_mae: 0.1388\n",
      "Epoch 85/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0358 - mse: 0.0358 - mae: 0.1392 - val_loss: 0.0356 - val_mse: 0.0356 - val_mae: 0.1386\n",
      "Epoch 86/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0358 - mse: 0.0358 - mae: 0.1391 - val_loss: 0.0399 - val_mse: 0.0399 - val_mae: 0.1483\n",
      "Epoch 87/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0355 - mse: 0.0355 - mae: 0.1386 - val_loss: 0.0354 - val_mse: 0.0354 - val_mae: 0.1384\n",
      "Epoch 88/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0353 - mse: 0.0353 - mae: 0.1381 - val_loss: 0.0343 - val_mse: 0.0343 - val_mae: 0.1372\n",
      "Epoch 89/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0351 - mse: 0.0351 - mae: 0.1378 - val_loss: 0.0329 - val_mse: 0.0329 - val_mae: 0.1342\n",
      "Epoch 90/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0353 - mse: 0.0353 - mae: 0.1380 - val_loss: 0.0380 - val_mse: 0.0380 - val_mae: 0.1442\n",
      "Epoch 91/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0349 - mse: 0.0349 - mae: 0.1375 - val_loss: 0.0346 - val_mse: 0.0346 - val_mae: 0.1367\n",
      "Epoch 92/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0347 - mse: 0.0347 - mae: 0.1370 - val_loss: 0.0325 - val_mse: 0.0325 - val_mae: 0.1321\n",
      "Epoch 93/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0347 - mse: 0.0347 - mae: 0.1367 - val_loss: 0.0396 - val_mse: 0.0396 - val_mae: 0.1463\n",
      "Epoch 94/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0343 - mse: 0.0343 - mae: 0.1360 - val_loss: 0.0344 - val_mse: 0.0344 - val_mae: 0.1366\n",
      "Epoch 95/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0345 - mse: 0.0345 - mae: 0.1366 - val_loss: 0.0357 - val_mse: 0.0357 - val_mae: 0.1400\n",
      "Epoch 96/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0346 - mse: 0.0346 - mae: 0.1366 - val_loss: 0.0341 - val_mse: 0.0341 - val_mae: 0.1359\n",
      "Epoch 97/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0342 - mse: 0.0342 - mae: 0.1357 - val_loss: 0.0429 - val_mse: 0.0429 - val_mae: 0.1534\n",
      "Epoch 98/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0338 - mse: 0.0338 - mae: 0.1351 - val_loss: 0.0334 - val_mse: 0.0334 - val_mae: 0.1345\n",
      "Epoch 99/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0341 - mse: 0.0341 - mae: 0.1356 - val_loss: 0.0491 - val_mse: 0.0491 - val_mae: 0.1559\n",
      "Epoch 100/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0339 - mse: 0.0339 - mae: 0.1351 - val_loss: 0.0328 - val_mse: 0.0328 - val_mae: 0.1331\n",
      "Epoch 101/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0338 - mse: 0.0338 - mae: 0.1350 - val_loss: 0.0334 - val_mse: 0.0334 - val_mae: 0.1353\n",
      "Epoch 102/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0332 - mse: 0.0332 - mae: 0.1339 - val_loss: 0.0328 - val_mse: 0.0328 - val_mae: 0.1334\n",
      "Epoch 103/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0336 - mse: 0.0336 - mae: 0.1345 - val_loss: 0.0383 - val_mse: 0.0383 - val_mae: 0.1442\n",
      "Epoch 104/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0335 - mse: 0.0335 - mae: 0.1344 - val_loss: 0.0332 - val_mse: 0.0332 - val_mae: 0.1347\n",
      "Epoch 105/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0334 - mse: 0.0334 - mae: 0.1342 - val_loss: 0.0330 - val_mse: 0.0330 - val_mae: 0.1344\n",
      "Epoch 106/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0327 - mse: 0.0327 - mae: 0.1328 - val_loss: 0.0362 - val_mse: 0.0362 - val_mae: 0.1407\n",
      "Epoch 107/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0329 - mse: 0.0329 - mae: 0.1331 - val_loss: 0.0429 - val_mse: 0.0429 - val_mae: 0.1507\n",
      "Epoch 108/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0328 - mse: 0.0328 - mae: 0.1329 - val_loss: 0.0407 - val_mse: 0.0407 - val_mae: 0.1468\n",
      "Epoch 109/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0329 - mse: 0.0329 - mae: 0.1331 - val_loss: 0.0367 - val_mse: 0.0367 - val_mae: 0.1402\n",
      "Epoch 110/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0328 - mse: 0.0328 - mae: 0.1328 - val_loss: 0.0322 - val_mse: 0.0322 - val_mae: 0.1320\n",
      "Epoch 111/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0324 - mse: 0.0324 - mae: 0.1320 - val_loss: 0.0392 - val_mse: 0.0392 - val_mae: 0.1435\n",
      "Epoch 112/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0323 - mse: 0.0323 - mae: 0.1320 - val_loss: 0.0330 - val_mse: 0.0330 - val_mae: 0.1349\n",
      "Epoch 113/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0323 - mse: 0.0323 - mae: 0.1317 - val_loss: 0.0335 - val_mse: 0.0335 - val_mae: 0.1346\n",
      "Epoch 114/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0324 - mse: 0.0324 - mae: 0.1320 - val_loss: 0.0337 - val_mse: 0.0337 - val_mae: 0.1353\n",
      "Epoch 115/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0322 - mse: 0.0322 - mae: 0.1315 - val_loss: 0.0304 - val_mse: 0.0304 - val_mae: 0.1284\n",
      "Epoch 116/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0322 - mse: 0.0322 - mae: 0.1314 - val_loss: 0.0351 - val_mse: 0.0351 - val_mae: 0.1392\n",
      "Epoch 117/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0317 - mse: 0.0317 - mae: 0.1306 - val_loss: 0.0320 - val_mse: 0.0320 - val_mae: 0.1318\n",
      "Epoch 118/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0319 - mse: 0.0319 - mae: 0.1309 - val_loss: 0.0328 - val_mse: 0.0328 - val_mae: 0.1336\n",
      "Epoch 119/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0316 - mse: 0.0316 - mae: 0.1304 - val_loss: 0.0329 - val_mse: 0.0329 - val_mae: 0.1338\n",
      "Epoch 120/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0316 - mse: 0.0316 - mae: 0.1304 - val_loss: 0.0330 - val_mse: 0.0330 - val_mae: 0.1338\n",
      "Epoch 121/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0315 - mse: 0.0315 - mae: 0.1301 - val_loss: 0.0317 - val_mse: 0.0317 - val_mae: 0.1308\n",
      "Epoch 122/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0315 - mse: 0.0315 - mae: 0.1301 - val_loss: 0.0383 - val_mse: 0.0383 - val_mae: 0.1446\n",
      "Epoch 123/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0310 - mse: 0.0310 - mae: 0.1292 - val_loss: 0.0309 - val_mse: 0.0309 - val_mae: 0.1300\n",
      "Epoch 124/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0312 - mse: 0.0312 - mae: 0.1293 - val_loss: 0.0312 - val_mse: 0.0312 - val_mae: 0.1304\n",
      "Epoch 125/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0313 - mse: 0.0313 - mae: 0.1296 - val_loss: 0.0311 - val_mse: 0.0311 - val_mae: 0.1301\n",
      "Epoch 126/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0310 - mse: 0.0310 - mae: 0.1291 - val_loss: 0.0299 - val_mse: 0.0299 - val_mae: 0.1273\n",
      "Epoch 127/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0310 - mse: 0.0310 - mae: 0.1290 - val_loss: 0.0346 - val_mse: 0.0346 - val_mae: 0.1375\n",
      "Epoch 128/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0310 - mse: 0.0310 - mae: 0.1290 - val_loss: 0.0323 - val_mse: 0.0323 - val_mae: 0.1322\n",
      "Epoch 129/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0307 - mse: 0.0307 - mae: 0.1284 - val_loss: 0.0312 - val_mse: 0.0312 - val_mae: 0.1293\n",
      "Epoch 130/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0308 - mse: 0.0308 - mae: 0.1285 - val_loss: 0.0373 - val_mse: 0.0373 - val_mae: 0.1416\n",
      "Epoch 131/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0304 - mse: 0.0304 - mae: 0.1278 - val_loss: 0.0404 - val_mse: 0.0404 - val_mae: 0.1464\n",
      "Epoch 132/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0304 - mse: 0.0304 - mae: 0.1278 - val_loss: 0.0335 - val_mse: 0.0335 - val_mae: 0.1352\n",
      "Epoch 133/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0304 - mse: 0.0304 - mae: 0.1278 - val_loss: 0.0352 - val_mse: 0.0352 - val_mae: 0.1397\n",
      "Epoch 134/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0302 - mse: 0.0302 - mae: 0.1274 - val_loss: 0.0307 - val_mse: 0.0307 - val_mae: 0.1290\n",
      "Epoch 135/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0304 - mse: 0.0304 - mae: 0.1276 - val_loss: 0.0299 - val_mse: 0.0299 - val_mae: 0.1275\n",
      "Epoch 136/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0301 - mse: 0.0301 - mae: 0.1272 - val_loss: 0.0312 - val_mse: 0.0312 - val_mae: 0.1298\n",
      "Epoch 137/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0302 - mse: 0.0302 - mae: 0.1273 - val_loss: 0.0302 - val_mse: 0.0302 - val_mae: 0.1280\n",
      "Epoch 138/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0300 - mse: 0.0300 - mae: 0.1270 - val_loss: 0.0344 - val_mse: 0.0344 - val_mae: 0.1364\n",
      "Epoch 139/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0299 - mse: 0.0299 - mae: 0.1266 - val_loss: 0.0290 - val_mse: 0.0290 - val_mae: 0.1258\n",
      "Epoch 140/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0301 - mse: 0.0301 - mae: 0.1270 - val_loss: 0.0312 - val_mse: 0.0312 - val_mae: 0.1303\n",
      "Epoch 141/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0297 - mse: 0.0297 - mae: 0.1263 - val_loss: 0.0315 - val_mse: 0.0315 - val_mae: 0.1308\n",
      "Epoch 142/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0298 - mse: 0.0298 - mae: 0.1262 - val_loss: 0.0301 - val_mse: 0.0301 - val_mae: 0.1281\n",
      "Epoch 143/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0296 - mse: 0.0296 - mae: 0.1260 - val_loss: 0.0344 - val_mse: 0.0344 - val_mae: 0.1362\n",
      "Epoch 144/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0294 - mse: 0.0294 - mae: 0.1256 - val_loss: 0.0303 - val_mse: 0.0303 - val_mae: 0.1282\n",
      "Epoch 145/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0296 - mse: 0.0296 - mae: 0.1259 - val_loss: 0.0326 - val_mse: 0.0326 - val_mae: 0.1315\n",
      "Epoch 146/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0292 - mse: 0.0292 - mae: 0.1250 - val_loss: 0.0318 - val_mse: 0.0318 - val_mae: 0.1307\n",
      "Epoch 147/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0295 - mse: 0.0295 - mae: 0.1257 - val_loss: 0.0282 - val_mse: 0.0282 - val_mae: 0.1235\n",
      "Epoch 148/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0290 - mse: 0.0290 - mae: 0.1247 - val_loss: 0.0291 - val_mse: 0.0291 - val_mae: 0.1255\n",
      "Epoch 149/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0291 - mse: 0.0291 - mae: 0.1248 - val_loss: 0.0279 - val_mse: 0.0279 - val_mae: 0.1233\n",
      "Epoch 150/200\n",
      "3270/3270 [==============================] - 25s 7ms/step - loss: 0.0292 - mse: 0.0292 - mae: 0.1249 - val_loss: 0.0331 - val_mse: 0.0331 - val_mae: 0.1340\n",
      "Epoch 151/200\n",
      "3270/3270 [==============================] - 28s 8ms/step - loss: 0.0290 - mse: 0.0290 - mae: 0.1246 - val_loss: 0.0286 - val_mse: 0.0286 - val_mae: 0.1250\n",
      "Epoch 152/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0289 - mse: 0.0289 - mae: 0.1243 - val_loss: 0.0281 - val_mse: 0.0281 - val_mae: 0.1240\n",
      "Epoch 153/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0289 - mse: 0.0289 - mae: 0.1245 - val_loss: 0.0280 - val_mse: 0.0280 - val_mae: 0.1223\n",
      "Epoch 154/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0287 - mse: 0.0287 - mae: 0.1241 - val_loss: 0.0289 - val_mse: 0.0289 - val_mae: 0.1251\n",
      "Epoch 155/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0287 - mse: 0.0287 - mae: 0.1240 - val_loss: 0.0315 - val_mse: 0.0315 - val_mae: 0.1313\n",
      "Epoch 156/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0289 - mse: 0.0289 - mae: 0.1244 - val_loss: 0.0271 - val_mse: 0.0271 - val_mae: 0.1211\n",
      "Epoch 157/200\n",
      "3270/3270 [==============================] - 25s 7ms/step - loss: 0.0288 - mse: 0.0288 - mae: 0.1241 - val_loss: 0.0348 - val_mse: 0.0348 - val_mae: 0.1380\n",
      "Epoch 158/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0286 - mse: 0.0286 - mae: 0.1237 - val_loss: 0.0296 - val_mse: 0.0296 - val_mae: 0.1271\n",
      "Epoch 159/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0285 - mse: 0.0285 - mae: 0.1235 - val_loss: 0.0275 - val_mse: 0.0275 - val_mae: 0.1217\n",
      "Epoch 160/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0284 - mse: 0.0284 - mae: 0.1233 - val_loss: 0.0407 - val_mse: 0.0407 - val_mae: 0.1455\n",
      "Epoch 161/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0282 - mse: 0.0282 - mae: 0.1228 - val_loss: 0.0274 - val_mse: 0.0274 - val_mae: 0.1212\n",
      "Epoch 162/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0282 - mse: 0.0282 - mae: 0.1229 - val_loss: 0.0291 - val_mse: 0.0291 - val_mae: 0.1251\n",
      "Epoch 163/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0281 - mse: 0.0281 - mae: 0.1227 - val_loss: 0.0281 - val_mse: 0.0281 - val_mae: 0.1236\n",
      "Epoch 164/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0281 - mse: 0.0281 - mae: 0.1227 - val_loss: 0.0309 - val_mse: 0.0309 - val_mae: 0.1295\n",
      "Epoch 165/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0282 - mse: 0.0282 - mae: 0.1228 - val_loss: 0.0300 - val_mse: 0.0300 - val_mae: 0.1267\n",
      "Epoch 166/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0278 - mse: 0.0278 - mae: 0.1220 - val_loss: 0.0314 - val_mse: 0.0314 - val_mae: 0.1296\n",
      "Epoch 167/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0281 - mse: 0.0281 - mae: 0.1226 - val_loss: 0.0282 - val_mse: 0.0282 - val_mae: 0.1241\n",
      "Epoch 168/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0278 - mse: 0.0278 - mae: 0.1220 - val_loss: 0.0290 - val_mse: 0.0290 - val_mae: 0.1252\n",
      "Epoch 169/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0280 - mse: 0.0280 - mae: 0.1223 - val_loss: 0.0352 - val_mse: 0.0352 - val_mae: 0.1371\n",
      "Epoch 170/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0280 - mse: 0.0280 - mae: 0.1224 - val_loss: 0.0306 - val_mse: 0.0306 - val_mae: 0.1288\n",
      "Epoch 171/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0279 - mse: 0.0279 - mae: 0.1220 - val_loss: 0.0339 - val_mse: 0.0339 - val_mae: 0.1343\n",
      "Epoch 172/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0275 - mse: 0.0275 - mae: 0.1213 - val_loss: 0.0302 - val_mse: 0.0302 - val_mae: 0.1273\n",
      "Epoch 173/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0277 - mse: 0.0277 - mae: 0.1216 - val_loss: 0.0304 - val_mse: 0.0304 - val_mae: 0.1274\n",
      "Epoch 174/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0274 - mse: 0.0274 - mae: 0.1211 - val_loss: 0.0296 - val_mse: 0.0296 - val_mae: 0.1273\n",
      "Epoch 175/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0274 - mse: 0.0274 - mae: 0.1210 - val_loss: 0.0274 - val_mse: 0.0274 - val_mae: 0.1218\n",
      "Epoch 176/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0275 - mse: 0.0275 - mae: 0.1213 - val_loss: 0.0379 - val_mse: 0.0379 - val_mae: 0.1402\n",
      "Epoch 177/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0273 - mse: 0.0273 - mae: 0.1209 - val_loss: 0.0362 - val_mse: 0.0362 - val_mae: 0.1393\n",
      "Epoch 178/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0275 - mse: 0.0275 - mae: 0.1212 - val_loss: 0.0271 - val_mse: 0.0271 - val_mae: 0.1219\n",
      "Epoch 179/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0273 - mse: 0.0273 - mae: 0.1207 - val_loss: 0.0282 - val_mse: 0.0282 - val_mae: 0.1240\n",
      "Epoch 180/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0273 - mse: 0.0273 - mae: 0.1207 - val_loss: 0.0270 - val_mse: 0.0270 - val_mae: 0.1216\n",
      "Epoch 181/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0271 - mse: 0.0271 - mae: 0.1203 - val_loss: 0.0309 - val_mse: 0.0309 - val_mae: 0.1292\n",
      "Epoch 182/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0272 - mse: 0.0272 - mae: 0.1206 - val_loss: 0.0290 - val_mse: 0.0290 - val_mae: 0.1253\n",
      "Epoch 183/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0271 - mse: 0.0271 - mae: 0.1204 - val_loss: 0.0273 - val_mse: 0.0273 - val_mae: 0.1216\n",
      "Epoch 184/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0269 - mse: 0.0269 - mae: 0.1200 - val_loss: 0.0253 - val_mse: 0.0253 - val_mae: 0.1171\n",
      "Epoch 185/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0269 - mse: 0.0269 - mae: 0.1200 - val_loss: 0.0267 - val_mse: 0.0267 - val_mae: 0.1202\n",
      "Epoch 186/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0270 - mse: 0.0270 - mae: 0.1201 - val_loss: 0.0289 - val_mse: 0.0289 - val_mae: 0.1246\n",
      "Epoch 187/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0269 - mse: 0.0269 - mae: 0.1198 - val_loss: 0.0271 - val_mse: 0.0271 - val_mae: 0.1210\n",
      "Epoch 188/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0269 - mse: 0.0269 - mae: 0.1198 - val_loss: 0.0455 - val_mse: 0.0455 - val_mae: 0.1515\n",
      "Epoch 189/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0268 - mse: 0.0268 - mae: 0.1196 - val_loss: 0.0269 - val_mse: 0.0269 - val_mae: 0.1206\n",
      "Epoch 190/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0268 - mse: 0.0268 - mae: 0.1197 - val_loss: 0.0355 - val_mse: 0.0355 - val_mae: 0.1349\n",
      "Epoch 191/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0265 - mse: 0.0265 - mae: 0.1189 - val_loss: 0.0251 - val_mse: 0.0251 - val_mae: 0.1166\n",
      "Epoch 192/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0269 - mse: 0.0269 - mae: 0.1199 - val_loss: 0.0324 - val_mse: 0.0324 - val_mae: 0.1313\n",
      "Epoch 193/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0265 - mse: 0.0265 - mae: 0.1190 - val_loss: 0.0273 - val_mse: 0.0273 - val_mae: 0.1217\n",
      "Epoch 194/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0263 - mse: 0.0263 - mae: 0.1186 - val_loss: 0.0281 - val_mse: 0.0281 - val_mae: 0.1226\n",
      "Epoch 195/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0267 - mse: 0.0267 - mae: 0.1193 - val_loss: 0.0291 - val_mse: 0.0291 - val_mae: 0.1259\n",
      "Epoch 196/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0263 - mse: 0.0263 - mae: 0.1185 - val_loss: 0.0270 - val_mse: 0.0270 - val_mae: 0.1201\n",
      "Epoch 197/200\n",
      "3270/3270 [==============================] - 24s 7ms/step - loss: 0.0265 - mse: 0.0265 - mae: 0.1191 - val_loss: 0.0342 - val_mse: 0.0342 - val_mae: 0.1332\n",
      "Epoch 198/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0263 - mse: 0.0263 - mae: 0.1186 - val_loss: 0.0313 - val_mse: 0.0313 - val_mae: 0.1292\n",
      "Epoch 199/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0263 - mse: 0.0263 - mae: 0.1185 - val_loss: 0.0277 - val_mse: 0.0277 - val_mae: 0.1221\n",
      "Epoch 200/200\n",
      "3270/3270 [==============================] - 25s 8ms/step - loss: 0.0263 - mse: 0.0263 - mae: 0.1184 - val_loss: 0.0340 - val_mse: 0.0340 - val_mae: 0.1330\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at C:\\Users\\zacha\\Documents\\GitHub\\plymouth-university-proj518\\saved_models\\wind_time_regression_model_25082022164000.h5 \n",
      "12916/12916 [==============================] - 26s 2ms/step - loss: 0.0341 - mse: 0.0341 - mae: 0.1329\n",
      "Test loss: 0.0340682677924633\n",
      "Test accuracy: 0.0340682677924633\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 840,465\n",
      "Trainable params: 840,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8296\\3405374427.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mtest_coords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m37.814\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m144.96332\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m37.814\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m144.96332\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m103\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mtest_coords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Lon\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Lat\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Day\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_coords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8296\\47400494.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(model, coords)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mregression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     regression = model.inverse_transform(\n\u001b[0;32m      4\u001b[0m         \u001b[0mregression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     )  # transform back to original scale\n",
      "\u001b[1;32mc:\\Users\\zacha\\miniconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zacha\\miniconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zacha\\miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[0mTransformed\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m         \"\"\"\n\u001b[1;32m--> 970\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zacha\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfitted\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1222\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# build model\n",
    "x_train, y_train, x_test, y_test, yscaler = scaler()\n",
    "model = build()\n",
    "\n",
    "# train model\n",
    "model = train(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    epochs=200,  # test 1-5: 150, test 6: 200\n",
    "    batch_size=1024,  # test 1-5: 50, test 6: 128\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    ")\n",
    "\n",
    "# print model summary - architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Lon        Lat  Day           Chi\n",
      "0 -37.814  144.96332    1 -5.142513e+06\n",
      "1 -37.814  144.96332  103  1.716039e+05\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "test_coords = pd.DataFrame([[-37.814, 144.96332, 1], [-37.814, 144.96332, 103]])\n",
    "test_coords.columns = [\"Lon\", \"Lat\", \"Day\"]\n",
    "test(model, yscaler, coords=test_coords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1490a734e1ed5a586e2dbc3fd70efa60aaa0bba665dd1e0af63a448fbcaeb9e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
